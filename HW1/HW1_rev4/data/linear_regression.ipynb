{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 545 (WN 2024) Assignment 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a simple linear regression algorithm covered in the Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "\n",
    "display_html(HTML('''\n",
    "<style type=\"text/css\">\n",
    "  .instruction { background-color: yellow; font-weight:bold; padding: 3px; }\n",
    "</style>\n",
    "'''));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGELOG\n",
    "\n",
    "- rev0: Initial release (24. 1. 10. 1PM)\n",
    "- rev4: Removed import ipdb; ipdb.set_trace() in Q3d closed_form_locally_weighted function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"instruction\">Before starting the assignment, please fill in the following cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Enter your first and last name, e.g. \"John Doe\"                 #\n",
    "# for example,                                                    #\n",
    "# __NAME__ = \"Honglak Lee\"                                        #\n",
    "# __UNIQID__ = \"honglak\"                                          #\n",
    "###################################################################\n",
    "__NAME__ = \"Yuzhou Chen\"\n",
    "__UNIQID__ = \"yzc\"\n",
    "raise NotImplementedError(\"TODO: Add your implementation here.\")\n",
    "###################################################################\n",
    "#                        END OF YOUR CODE                         #\n",
    "###################################################################\n",
    "\n",
    "print(f\"Your name and email: {__NAME__} <{__UNIQID__}@umich.edu>\")\n",
    "assert __NAME__ and __UNIQID__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to have some boilerplate code to set up this notebook. First, run the following cell which loads the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) IPython magic: this allows us to automatically import and hotload the source files you will be modifying (e.g. `linear_regression.py`) into the notebook for a seamless editing and debugging expreience.\n",
    "\n",
    "NOTE: It is usually a good practice to implement code (\"business logic\" or core computation) as a \"library\", usually implemented with a python module, which can be imported from ipython(jupyter) notebook rather than implementing everything in jupyter notebook cells. This practice would help make your code more testable and well-structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More boilerplates follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that are needed in this assignment.\n",
    "# Note: You should not import or use other packages than numpy and matplotlib that would\n",
    "# trivialize your work. Actually, you will never need to import them.\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If you run into ImportError (ModuleNotFoundError), uncomment and execute the following line.\n",
    "# !pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the python module you are going to implement. See linear_regression.py\n",
    "import linear_regression\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "# Dump environment information\n",
    "print(\"Current time:\", datetime.datetime.now())\n",
    "print(f\"python: {sys.version}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "\n",
    "# Use high-resolution images for inline matplotlib possible whenever possible\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Default params for plot\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1. Implementing GD and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the following cell to load the sample data provided. We also generate the feature vector $\\phi(\\mathbf{x}^{(i)})$ for each data point $\\mathbf x^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = linear_regression.load_data()\n",
    "X_train = linear_regression.generate_polynomial_features(x_train, M=1)\n",
    "X_test = linear_regression.generate_polynomial_features(x_test, M=1)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a)-1: Batch Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"instruction\">Now, implement the iterative batch gradient descent method</span> in the `linear_regression.batch_grad()` function. The function prototype is given as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.batch_gradient_descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are required to implement `linear_regression.compute_objective()` function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.compute_objective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's run the GD solver with the provided sample data, fitting the training set. The function should return a 2-tuple `(w, info)`, where:\n",
    "\n",
    "- `w` : The coefficients generated by the optimization method.\n",
    "- `info`: The additional informations. In this problem, we will store 'train_objectives' and 'convergence_iter'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gd, _ = linear_regression.batch_gradient_descent(X_train, y_train)\n",
    "train_objective = linear_regression.compute_objective(X_train, y_train, w=w_gd)\n",
    "\n",
    "print(f\"w = {w_gd}\")\n",
    "print(f\"Training objective after convergence = {train_objective}\")\n",
    "\n",
    "assert train_objective <= 4.0, \"If you implemented Batch GD correctly, it should be smaller than the given number.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also measure the test objective (error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_objective = linear_regression.compute_objective(X_test, y_test, w=w_gd)\n",
    "\n",
    "print(f\"w = {w_gd}\")\n",
    "print(f\"Test objective = {test_objective}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a)-2: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, <span class=\"instruction\">implement the iterative stochastic gradient descent method</span> in the `linear_regression.stochastic_gradient_descent()`, and let's repeat the same process with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sgd, _ = linear_regression.stochastic_gradient_descent(X_train, y_train)\n",
    "train_objective = linear_regression.compute_objective(X_train, y_train, w=w_sgd)\n",
    "\n",
    "print(f\"w = {w_sgd}\")\n",
    "print(f\"Training objective after convergence = {train_objective}\")\n",
    "\n",
    "assert train_objective <= 4.0, \"If you implemented Stochastic GD correctly, it should be smaller than the given number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_objective = linear_regression.compute_objective(X_test, y_test, w=w_sgd)\n",
    "\n",
    "print(f\"w = {w_sgd}\")\n",
    "print(f\"Test objective = {test_objective}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b): Drawing Learning Curves for GD and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compare two optimizations in terms of the number of epochs required to converge, the time spent for the computation.\n",
    " \n",
    "In this problem, we will use the additional information and statistics stored in `info`\n",
    "from `batch_gradient_descent` and `stochastic_gradient_descent`. Note that the use of `info` dictionary is a very common practice in ML implementations (e.g. [torch.grad](https://pytorch.org/functorch/stable/generated/functorch.grad.html?highlight=has_aux), [jax.grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html?highlight=has_aux)). \n",
    "\n",
    "Please execute the following cell, and report the plot with the time spent for each method in your **writeup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "eta = 0.01\n",
    "\n",
    "tic = time.time()\n",
    "w_gd, info_gd = linear_regression.batch_gradient_descent(X_train, y_train, eta=eta)\n",
    "toc = time.time()\n",
    "gd_time = toc - tic\n",
    "print(f'GD version took {gd_time:.2f} seconds')\n",
    "\n",
    "gd_test = linear_regression.compute_objective(X_test, y_test, w=w_gd)\n",
    "print(f\"GD Test objective = {gd_test:.4f}\")\n",
    "\n",
    "w_sgd, info_sgd = linear_regression.stochastic_gradient_descent(X_train, y_train, eta=eta)\n",
    "toc = time.time()\n",
    "sgd_time = toc - tic\n",
    "print(f'SGD version took {sgd_time:.2f} seconds')\n",
    "\n",
    "sgd_test = linear_regression.compute_objective(X_test, y_test, w=w_sgd)\n",
    "print(f\"SGD Test objective = {sgd_test:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(info_gd[\"train_objectives\"], linewidth=2.0, marker='o', markersize=4, label='Batch GD')\n",
    "ax.plot(info_sgd[\"train_objectives\"], linewidth=2.0, marker='x', markersize=4, label='Stochastic GD')\n",
    "\n",
    "# NOTE: It is always a good practice to include label and title for matplotlib plots.\n",
    "ax.set_title(\"Batch GD v.s. SGD\")\n",
    "ax.set_ylabel(\"Training Objective (loss)\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Overfitting Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c): Implementing the closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will investigate the problem of overfitting. Recall the figure from the Lecture 2~3, which explores over-fitting as a function of the degree of the polynomial $M$.\n",
    "\n",
    "As instructed in the problem set, we will use the closed form solution of linear regression instead of iterative optimizations.\n",
    "<span class=\"instruction\">Implement the function `linear_regression.closed_form()`, which gives the optimal solution of the linear regression</span> that minimizes the following loss function:\n",
    "\n",
    "$$\n",
    "E(\\mathbf w) = \\frac{1}{2} \\sum_{i=1}^N \\left( \\sum_{j=0}^M w_j \\phi_j(x^{(i)}) - y^{(i)} \\right)^2 = \\frac{1}{2} \\sum_{i=1}^N ({\\mathbf w}^\\top \\phi(x^{(i)}) - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.closed_form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(d): Plot generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the parameters, use the closed form solution of linear regression (assuming all the condition is met) that minimize the error for a $M$-degree polynomial (for $M = 0, \\ldots, 9$) for the training data `x_train` (q2xTrain.npy) and `y_train` (q2yTrain.npy). For the test curve, use the data in `x_test` (q2xTest.npy) and `y_test` (q2yTest.npy)\n",
    "\n",
    "Note: For different values of $M$, we assume the feature vector is\n",
    "$$\n",
    "    \\phi(x^{(i)}) = \\big[ 1, x^{(i)}, (x^{(i)})^2, \\cdots, (x^{(i)})^M \\big]\n",
    "$$\n",
    "for each $x^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can get the coefficient $\\mathbf{w}$ of `closed_form` for M=9 as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = linear_regression.generate_polynomial_features(x_train, M=9)\n",
    "w_closed = linear_regression.closed_form(X_train, y_train)\n",
    "print(f\"w_closed = {w_closed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to plot the chart as instructed in the problemset: plot $E_\\text{RMS}$ (y-axis) over $M=0,1,\\cdots,8,9$ (x-axis).\n",
    "<span class=\"instruction\">Complete the function `linear_regression.compute_rms_for_m()`, which computes the train and the test RMS error of the closed_form function, for given M and lambda.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.compute_rms_for_m?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you implemented correctly, you should be able to see a plot similar to the one in the problemset. Please attach your plot to your **writeup** and answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "M = 9\n",
    "\n",
    "train_rms_errors = []\n",
    "test_rms_errors = []\n",
    "for M_candidate in range(1, M+1):\n",
    "    train_rms_error, test_rms_error = linear_regression.compute_rms_for_m(\n",
    "                                      x_train, y_train, x_test, y_test, M_candidate)\n",
    "    \n",
    "    train_rms_errors.append(train_rms_error)\n",
    "    test_rms_errors.append(test_rms_error)\n",
    "\n",
    "ax.plot(np.arange(1, M + 1), train_rms_errors,\n",
    "        label='Train', color='b', marker='o', linewidth=2.0)\n",
    "ax.plot(np.arange(1, M + 1), test_rms_errors,\n",
    "        label='Test', color='r', marker='o', linewidth=2.0)\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"M\", ylabel=\"RMS Error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will find the coefficients that minimize the error for a $M=9$ degree polynimal given the regularization coefficient $\\lambda$, over $\\lambda \\in \\{0, 10^{-8}, 10^{-7}, 10^{-6}, \\cdots, 10^{-1}, 10^{0} (=1) \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(f). Regularization (ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"instruction\">Modify the implementation of `linear_regression.closed_form(...)` so that it takes the regularization coefficient $\\lambda$ into consideration.</span> The regularized object function is:\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^N (\\mathbf w^\\top \\phi(\\mathbf x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\| \\mathbf w \\|_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = linear_regression.generate_polynomial_features(x_train, M=9)\n",
    "X_test = linear_regression.generate_polynomial_features(x_test, M=9)\n",
    "w_closed_1 = linear_regression.closed_form(X_train, y_train, reg=1.0)\n",
    "print(f\"w_closed (lambda = 1) = {w_closed_1}\")\n",
    "\n",
    "w_closed_10 = linear_regression.closed_form(X_train, y_train, reg=10.0)\n",
    "print(f\"w_closed (lambda = 10) = {w_closed_10}\")\n",
    "\n",
    "assert np.any(w_closed_1 != w_closed_10), \"It should have different number once your function properly handle the lambda value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(g): Generate Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"instruction\">Draw a plot of the **RMS** error over different values of $\\log_{10} \\lambda$ (x-axis).</span>\n",
    "Be sure to plot the original (un-regularized) $E_\\text{RMS}$ when plotting, and also include legend. Use this result and plot to write your answer in the writeup. Which $\\lambda$ gives you the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "M = 9\n",
    "lambdas = np.array([0] + [pow(10, x) for x in [-5, -4, -3, -2, -1, 0]])\n",
    "\n",
    "train_rms_error_lambda = []\n",
    "test_rms_error_lambda = []\n",
    "for reg in lambdas:\n",
    "    train_rms_error, test_rms_error = linear_regression.compute_rms_for_m(\n",
    "                                      x_train, y_train, x_test, y_test, M, reg)\n",
    "    train_rms_error_lambda.append(train_rms_error)\n",
    "    test_rms_error_lambda.append(test_rms_error)\n",
    "\n",
    "assert train_rms_error_lambda[-1] != train_rms_error_lambda[-2], \"It should have different number if your compute_rms_for_m function properly handle the lambda value.\"\n",
    "assert test_rms_error_lambda[-1] != test_rms_error_lambda[-2], \"It should have different number if your compute_rms_for_m function properly handle the lambda value.\"\n",
    "\n",
    "log_lambdas = np.log10(lambdas + np.array([1e-6, *np.zeros(6)]))\n",
    "log_lambda_labels = [\"$-\\infty$\", *log_lambdas[1:]]\n",
    "\n",
    "ax.plot(log_lambdas, train_rms_error_lambda, c='blue', marker='o', label='Training')\n",
    "ax.plot(log_lambdas, test_rms_error_lambda, c='red', marker='o', label='Testing')\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "ax.set(xlabel=r\"$\\log_{10} \\lambda$\", ylabel=\"RMS Error\")\n",
    "\n",
    "log_lambdas = np.log10(lambdas + np.array([1e-6, *np.zeros(6)]))\n",
    "log_lambda_labels = [\"$-\\infty$\", *log_lambdas[1:]]\n",
    "ax.set_xticks(log_lambdas, labels=log_lambda_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Locally-weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cells, you will implement locally-weighted linear regression discussed in Q3(d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.(d).i <span class=\"instruction\"> Implement the following function:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.closed_form_locally_weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.(d).ii Generate Plot\n",
    "You can test and draw the requested plot as follows. In order to get the correct plot, you are required to implement `compute_y_space` function first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.compute_y_space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('data/q3x.npy')\n",
    "y_train = np.load('data/q3y.npy')\n",
    "X_train = linear_regression.generate_polynomial_features(x_train, M=1)\n",
    "\n",
    "K = 50\n",
    "# Scatter plot of data\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "ax.set_title(\"Locally weighted linear regression\")\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "ax.scatter(x_train, y_train, marker='.', c='black')\n",
    "\n",
    "#\n",
    "# The ordinary linear regression\n",
    "#\n",
    "w_linear = linear_regression.closed_form(X_train, y_train)\n",
    "\n",
    "x_space = np.linspace(x_train.min(), x_train.max(), num=K)\n",
    "ax.plot(x_space,\n",
    "        x_space * w_linear[1] + w_linear[0],\n",
    "        c='gray', linestyle='--', label='linear', linewidth=3)\n",
    "\n",
    "#\n",
    "# Locally-weighted linear regression\n",
    "#\n",
    "taus = [0.1, 0.3, 0.8, 2.0, 10.0]\n",
    "colors = ['red', 'orange', 'green', 'blue', 'magenta']\n",
    "\n",
    "for color, tau in zip(colors, taus):\n",
    "    y_space = linear_regression.compute_y_space(X_train, x_train, y_train, x_space, tau)\n",
    "    ax.plot(x_space, y_space,\n",
    "            c=color, label=f'tau={tau}', linewidth=2)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
