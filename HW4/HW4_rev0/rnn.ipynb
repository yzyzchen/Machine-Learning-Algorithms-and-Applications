{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# EECS 545 (WN 2024) Homework 4: RNNs and Image Captioning\n",
    "\n",
    "<span class=\"instruction\">Before starting the assignment, please fill in the following cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Enter your first and last name, e.g. \"John Doe\"                 #\n",
    "# for example                                                     #\n",
    "# __NAME__ = \"Anthony Liu\"                                        #\n",
    "# __UNIQID__ = \"anthliu\"                                          #\n",
    "###################################################################\n",
    "raise NotImplementedError(\"TODO: Add your implementation here.\")\n",
    "###################################################################\n",
    "#                        END OF YOUR CODE                         #\n",
    "###################################################################\n",
    "\n",
    "print(f\"Your name and email: {__NAME__} <{__UNIQID__}@umich.edu>\")\n",
    "assert __NAME__ and __UNIQID__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# RNNs and Image Captioning\n",
    "In this notebook, you will test your RNN implementation from `rnn.py` on the coco image captioning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYE9thuXn4zP"
   },
   "source": [
    "## Setup code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook. Let's start by checking whether we are using Python 3.11 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqEfH2Rpn9J3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3\")\n",
    "\n",
    "if sys.version_info[1] < 11:\n",
    "    print(\"Autograder will execute your code based on Python 3.11 environment. Please use Python 3.11 or higher to prevent any issues\")\n",
    "    print(\"You can create a conda environment with Python 3.11 like 'conda create --name eecs545 python=3.11'\")\n",
    "    raise Exception(\"Python 3 version is too low: {}\".format(sys.version))\n",
    "else:\n",
    "    print(\"You are good to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GaCqHOm9oPB3"
   },
   "source": [
    "Then, we run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oCaNVx6JoWid",
    "outputId": "2133e4c6-8a6e-4ea3-dd97-23ad471ba2b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "# !pip install numpy==1.24.1 matplotlib==3.6.2 scikit-learn==1.2.0 h5py==3.8.0 imageio==2.25.1\n",
    "\n",
    "# import libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set figure size\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3EvIZ0uAOVN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "\n",
    "display_html(HTML('''\n",
    "<style type=\"text/css\">\n",
    "  .instruction { background-color: yellow; font-weight:bold; padding: 3px; }\n",
    "</style>\n",
    "'''));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cell allow us to import from `rnn_layers.py` and `rnn.py`. If it works correctly, it should print the message:\n",
    "```Hello from rnn_layers.py``` and ```Hello from rnn.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rnn_layers import hello\n",
    "from rnn import hello as hello2\n",
    "hello()\n",
    "hello2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is COCO?\n",
    "\n",
    "[COCO](https://cocodataset.org/) (Common Objects in COntext) is a large-scale object detection, segmentation, and captioning dataset.\n",
    "\n",
    "COCO has 330K images (>200K labeled)! Labelled images have object segmentations, and captions.\n",
    "\n",
    "![COCO examples](https://cocodataset.org/images/coco-examples.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading COCO\n",
    "\n",
    "Please download the dataset (987Mb) from [this link](https://drive.google.com/file/d/1RjZTIVp4ES1Ewv1QAJgCS_7DYyJa-Yf1/view?usp=sharing) and unzip the folder into the data directory.\n",
    "```\n",
    "HW4 /\n",
    "--| image_captioning.ipynb\n",
    "--| data /\n",
    "----| coco_captioning /\n",
    "------| coco2014_captions.h5\n",
    "------| ...\n",
    "```\n",
    "\n",
    "The dataset contains the preprocessed features and captions from the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py\n",
    "import coco_utils\n",
    "\n",
    "MAX_TRAIN = 200# We'll train with a very small dataset\n",
    "\n",
    "# Load COCO data from disk; this returns a dictionary\n",
    "small_data = coco_utils.load_coco_data(max_train=MAX_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing COCO\n",
    "\n",
    "Let's check out some images in our COCO dataset. You can browse the full dataset [here](https://cocodataset.org/#explore)! My favourite image is [this one](https://cocodataset.org/#explore?id=314526). It turns out when you have a 330K image dataset, some of the images might be really [weird](https://blog.roboflow.com/coco-dataset-image-search/). Check it out!\n",
    "\n",
    "Let's visualize some images from the downloaded COCO. Note we have to load them from the URL since they aren't stored in our dataset. (We save the preprocessed features). Some of these images may not load properly because the URLs are no longer active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def md_table(urls, captions, cols=5):\n",
    "    table = '|   ' * cols + '|\\n' + '|---' * cols + '|\\n'\n",
    "    row = lambda idxs: '| ' + ' | '.join(f'![train image {i}]({urls[i]})' for i in idxs) + ' |'\n",
    "    row_cap = lambda idxs: '| ' + ' | '.join(f'**{str(captions[i])}**' for i in idxs) + ' |'\n",
    "    table += '\\n'.join(row(range(i, i+cols)) + '\\n' + row_cap(range(i, i+cols)) for i in range(0, len(urls), cols))\n",
    "    table = table.replace('<', '\\<')# show angle brackets in MD\n",
    "    table = table.replace('\\<br>', '<br>')# keep <br> as newline\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "n_imgs = 20\n",
    "rng = np.random.default_rng(545)\n",
    "captions, features, img_urls = coco_utils.sample_coco_minibatch(\n",
    "    small_data, split='train', batch_size=n_imgs, seed=545)\n",
    "cap_str = [coco_utils.decode_captions(c, small_data['idx_to_word']) for c in captions]\n",
    "md(md_table(img_urls, cap_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Footnote: Special tokens. \\<START> and \\<END> tokens denote the start and end of the captions.\n",
    "The \\<UNK> token is created during preprocessing and replaces words that do not occur in the dataset very often. Doing this sometimes helps the model learn better since it shrinks the vocabulary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Next, we'll train an RNN to predict captions on COCO. <span class=\"instruction\">Make sure you have implemented `temporal_fc_forward` and `temporal_fc_backward` in `rnn_layers.py`, and `CaptioningRNN` from `rnn.py`.</span>\n",
    "Read through `captioning_solver.py` to make sure you understand the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Checks\n",
    "\n",
    "Like `cnn.ipynb`, we have included some tests to check your implementation. However, we have left the numerical gradient checks blank.\n",
    "These checks are optional, but they may be helpful for debugging. Hint: follow the same steps as done in `cnn.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from rnn_layers import temporal_fc_backward, temporal_fc_forward\n",
    "\n",
    "# OPTIONAL TODO Test with eval_numerical_gradient_array.\n",
    "# Hint: Start by sampling some random test data. (Refer to the rnn_layers.py comments to check the shapes)\n",
    "# Then, call eval_numerical_gradient_array on the test data. (Refer to cnn.ipynb for examples)\n",
    "# Compare the results of eval_numerical_gradient_array with your own backward implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from rnn import CaptioningRNN\n",
    "\n",
    "# Generate dummy test data\n",
    "wtoi = {f'{i}': i for i in range(7)}# dummy word to index\n",
    "wtoi['<NULL>'] = 8\n",
    "wtoi['<START>'] = 9\n",
    "wtoi['<END>'] = 10\n",
    "np.random.seed(0)\n",
    "test_rnn_model = CaptioningRNN(\n",
    "      cell_type='rnn',\n",
    "      word_to_idx=wtoi,\n",
    "      input_dim=5,\n",
    "      hidden_dim=4,\n",
    "      wordvec_dim=5,\n",
    ")\n",
    "rng = np.random.default_rng(545)\n",
    "X = rng.standard_normal((2, 5))\n",
    "y = rng.integers(0, 10, size=(2, 4))\n",
    "\n",
    "print('Testing training loss...')\n",
    "loss, grads = test_rnn_model.loss(X, y)\n",
    "\n",
    "assert np.abs(loss - 7.783337014604364) < 1e-4, 'loss should be close.'\n",
    "\n",
    "# OPTIONAL TODO Test with eval_numerical_gradient_array.\n",
    "# Hint: write a loop to check each grad (Refer to cnn.ipynb for examples)\n",
    "\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dummy test data\n",
    "wtoi = {f'{i}': i for i in range(7)}# dummy word to index\n",
    "wtoi['<NULL>'] = 8\n",
    "wtoi['<START>'] = 9\n",
    "wtoi['<END>'] = 10\n",
    "np.random.seed(1)\n",
    "test_rnn_model = CaptioningRNN(\n",
    "      cell_type='rnn',\n",
    "      word_to_idx=wtoi,\n",
    "      input_dim=5,\n",
    "      hidden_dim=4,\n",
    "      wordvec_dim=5,\n",
    ")\n",
    "rng = np.random.default_rng(545)\n",
    "X = rng.standard_normal((2, 5))\n",
    "\n",
    "print('Testing rnn sample...')\n",
    "captions = test_rnn_model.sample(X, max_length=17)\n",
    "gt_captions = np.array([[1, 0, 7, 2, 1, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
    "       [6, 5, 1, 0, 7, 6, 5, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7]])\n",
    "assert np.all(captions == gt_captions)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import CaptioningRNN\n",
    "from captioning_solver import CaptioningSolver\n",
    "\n",
    "np.random.seed(0)\n",
    "# Experiment with vanilla RNN\n",
    "small_rnn_model = CaptioningRNN(\n",
    "      cell_type='rnn',\n",
    "      word_to_idx=small_data['word_to_idx'],\n",
    "      input_dim=small_data['train_features'].shape[1],\n",
    "      hidden_dim=512,\n",
    "      wordvec_dim=256,\n",
    ")\n",
    "\n",
    "small_rnn_solver = CaptioningSolver(small_rnn_model, small_data,\n",
    "       update_rule='adam',\n",
    "       num_epochs=58,\n",
    "       batch_size=25,\n",
    "       optim_config={\n",
    "         'learning_rate': 4e-3,\n",
    "       },\n",
    "       lr_decay=0.95,\n",
    "       verbose=True, print_every=10,\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!\n",
    "\n",
    "Remember to include the plot of your learning curves in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rnn_solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training losses\n",
    "plt.plot(small_rnn_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.savefig('image_captioning_loss.png', dpi=256)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Visualization\n",
    "\n",
    "Finally, let's test our model on some validation data. Include your generated train and validation captions in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import image_from_url\n",
    "\n",
    "tables = {}\n",
    "for split in ['train', 'val']:\n",
    "    # some images might be deprecated. You may rerun the code several times\n",
    "    # to successfully get the sample images from url.\n",
    "    minibatch = coco_utils.sample_coco_minibatch(\n",
    "        small_data, split=split, batch_size=10, seed=545)\n",
    "    gt_captions, features, urls = minibatch\n",
    "    gt_captions = coco_utils.decode_captions(gt_captions,\n",
    "                                             small_data['idx_to_word'])\n",
    "\n",
    "    sample_captions = small_rnn_model.sample(features)\n",
    "    sample_captions = coco_utils.decode_captions(sample_captions,\n",
    "                                                 small_data['idx_to_word'])\n",
    "\n",
    "    figure_caption = [f'Sample: {sample_cap} <br> GT: {gt_cap}' for sample_cap, gt_cap in zip(sample_captions, gt_captions)]\n",
    "    tables[split] = md_table(urls, figure_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md('## Training samples \\n ' + tables['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md('## Validation samples \\n ' + tables['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eYE9thuXn4zP",
    "CdowvtJen-IP",
    "KtMy3qeipNK3",
    "Hbe3wUpVAjma",
    "lJqim3P1qZgv",
    "ZLdCF3B-AOVT",
    "7XNJ3ydEAOVW",
    "vExP-7n3AOVa",
    "LjAUalCBAOVd",
    "8cPIajWNAOVg",
    "_CsYAv3uAOVi",
    "ixxgq5RKAOVl",
    "OlVbXxmPNzPY",
    "rDNZ8ZAnN7hj",
    "QpSrK3olUfOZ",
    "3zFWkxebWXtu",
    "mVCEro4FAOVq",
    "UG56gKWsAOVv",
    "37R_J2uMP3d-"
   ],
   "name": "two_layer_net.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
