{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# EECS 545 (WN 2024) Homework 3 Q5: Two Layer Net\n",
    "\n",
    "<span class=\"instruction\">Before starting the assignment, please fill in the following cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Enter your first and last name, e.g. \"John Doe\"                 #\n",
    "# for example                                                     #\n",
    "# __NAME__ = \"Anthony Liu\"                                        #\n",
    "# __UNIQID__ = \"anthliu\"                                          #\n",
    "###################################################################\n",
    "raise NotImplementedError(\"TODO: Add your implementation here.\")\n",
    "###################################################################\n",
    "#                        END OF YOUR CODE                         #\n",
    "###################################################################\n",
    "\n",
    "print(f\"Your name and email: {__NAME__} <{__UNIQID__}@umich.edu>\")\n",
    "assert __NAME__ and __UNIQID__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# Implementing a Two-layer Neural Network\n",
    "In this notebook, you will implement a neural network with two fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
    "\n",
    "After implementing the two-layer neural network, you will report the training loss/accuracy plot and the test performance on MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYE9thuXn4zP"
   },
   "source": [
    "## Setup code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook. Let's start by checking whether we are using Python 3.11 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqEfH2Rpn9J3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3\")\n",
    "\n",
    "if sys.version_info[1] < 11:\n",
    "    print(\"Autograder will execute your code based on Python 3.11 environment. Please use Python 3.11 or higher to prevent any issues\")\n",
    "    print(\"You can create a conda environment with Python 3.11 like 'conda create --name eecs545 python=3.11'\")\n",
    "    raise Exception(\"Python 3 version is too low: {}\".format(sys.version))\n",
    "else:\n",
    "    print(\"You are good to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you located the `two_layer_net.py` correctly, run the following cell allow us to import from `two_layer_net.py`. If it works correctly, it should print the message:\n",
    "```Hello from two_layer_net.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from two_layer_net import hello\n",
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GaCqHOm9oPB3"
   },
   "source": [
    "Then, we run some setup code for this notebook: Increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3EvIZ0uAOVN"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "\n",
    "display_html(HTML('''\n",
    "<style type=\"text/css\">\n",
    "  .instruction { background-color: yellow; font-weight:bold; padding: 3px; }\n",
    "</style>\n",
    "'''));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We additionally load a toy dataset, which helps us to debug your initial implementation of forward and backward path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In this exercise we will implement fully-connected networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLdCF3B-AOVT"
   },
   "source": [
    "### Forward pass with a toy dataset\n",
    "\n",
    "First we will implement the forward pass of the network which uses the weights and biases to compute scores for all inputs.\n",
    "\n",
    "<span class=\"instruction\">Please complete the forward pass code in `fc_forward` of `two_layer_net.py`</span>. The distance gap between your output and the answer should be smaller than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "tZV9_3ZWAOVU",
    "outputId": "7504b688-c002-4676-c064-29adc38f88a4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gradient_check import rel_error\n",
    "from two_layer_net import fc_forward\n",
    "\n",
    "num_inputs = 2\n",
    "dim = 120\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * dim\n",
    "weight_size = output_dim * dim\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, dim)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(dim, output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = fc_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error might be less than 1e-9.\n",
    "# As long as your error is small enough, your implementation should pass this test.\n",
    "print('Testing fc_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))\n",
    "print()\n",
    "np.testing.assert_allclose(out, correct_out, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XNJ3ydEAOVW"
   },
   "source": [
    "### Backward pass\n",
    "Now, we implement `fc_backward` function and test your implementation using numeric gradient checking.\n",
    "\n",
    "<span class=\"instruction\">Please complete `fc_backward` of `two_layer_net.py`</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wgG6w2uKAOVX",
    "outputId": "e198ce0f-1f05-431e-e724-240aeaa09b5e"
   },
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient_array\n",
    "from two_layer_net import fc_forward, fc_backward\n",
    "\n",
    "# Test the affine_backward function\n",
    "np.random.seed(545)\n",
    "x = np.random.randn(10, 6)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = fc_forward(x, w, b)\n",
    "dx, dw, db = fc_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('\\nTesting fc_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "print()\n",
    "\n",
    "np.testing.assert_allclose(dx, dx_num, atol=1e-8)\n",
    "np.testing.assert_allclose(dw, dw_num, atol=1e-8)\n",
    "np.testing.assert_allclose(db, db_num, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vExP-7n3AOVa"
   },
   "source": [
    "### ReLU activation: forward\n",
    "\n",
    "We would like to use ReLU as the intermediate nonlinearity function.\n",
    "<span class=\"instruction\">Please implement the forward pass for the ReLU activation function in the `relu_forward` function in `two_layer_net.py`</span>. \n",
    "Once you implement this function, you can test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qCEkprvoAOVb",
    "outputId": "3e02d110-e672-4e33-80b8-5d898cfb30ef"
   },
   "outputs": [],
   "source": [
    "from gradient_check import rel_error\n",
    "from two_layer_net import relu_forward\n",
    "\n",
    "# ReLU layer: forward\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error might be around 5e-8\n",
    "# As long as your error is small enough, your implementation should pass this test.\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))\n",
    "print()\n",
    "\n",
    "np.testing.assert_allclose(out, correct_out, atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation: backward\n",
    "Now, we implement the backward pass for the ReLU activation function.\n",
    "<span class=\"instruction\"> Please complete `relu_backward` function in `two_layer_net.py`</span>. \n",
    "You can test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient_array\n",
    "from two_layer_net import relu_forward, relu_backward\n",
    "\n",
    "# ReLU layer: backward\n",
    "np.random.seed(545)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 3e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print()\n",
    "np.testing.assert_allclose(dx, dx_num, atol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, fully connected layer are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define `fc_relu_forward` and `fc_relu_backward` functions below.\n",
    "\n",
    "For now take a look at the `fc_relu_forward` and `fc_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient_array\n",
    "from two_layer_net import fc_forward, relu_forward, fc_backward, relu_backward\n",
    "\n",
    "\n",
    "def fc_relu_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Convenience layer that perorms an affine transform followed by a ReLU\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, fc_cache = fc_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def fc_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the affine-relu convenience layer\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = fc_backward(da, fc_cache)\n",
    "    return dx, dw, db\n",
    "\n",
    "\n",
    "# Sandwich layer test\n",
    "np.random.seed(545)\n",
    "x = np.random.randn(10, 6)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "out, cache = fc_relu_forward(x, w, b)\n",
    "dx, dw, db = fc_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be less than e-9\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss layers: Softmax\n",
    "Now implement the loss and gradient for Softmax. <span class=\"instruction\">Please implement `softmax_loss` function in `two_layers_net.py`.</span> You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient\n",
    "from two_layer_net import softmax_loss\n",
    "\n",
    "# Softmax loss\n",
    "np.random.seed(545)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be around 2.3 and dx error might be around 8e-9\n",
    "# As long as your error is small enough, your implementation should pass this test.\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print()\n",
    "\n",
    "np.testing.assert_allclose(loss, 2.3, atol=0.05)\n",
    "np.testing.assert_allclose(dx, dx_num, atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-layer network\n",
    "As we have all component, <span class=\"instruction\">now we would like to implement the `TwoLayerNet` class in `two_layers_net.py'.</span> Read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(545)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "# relative error should be less than 1e-7\n",
    "for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cPIajWNAOVg",
    "tags": []
   },
   "source": [
    "## Let's test our model with a dataset: MNIST\n",
    "MNIST is commonly used for training various image processing systems. This dataset stores handwritten digits with matched number as label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "lYo_XrU3AOVg",
    "outputId": "e0e8ca93-3570-45f4-96ec-d03d91b1148c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_grid(array, ncols=3):\n",
    "    nindex, height, width, intensity = array.shape\n",
    "    nrows = nindex // ncols\n",
    "    assert nindex == nrows * ncols\n",
    "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
    "              .swapaxes(1,2)\n",
    "              .reshape(height * nrows, width * ncols, intensity))\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_mnist(path, visualize=True, fully_connected=True):\n",
    "    mnist = np.load(path)\n",
    "    x_train = mnist['x_train']\n",
    "    y_train = mnist['y_train']\n",
    "    x_test = mnist['x_test']\n",
    "    y_test = mnist['y_test']\n",
    "    \n",
    "    num_test = len(y_test)\n",
    "    \n",
    "    if visualize:\n",
    "        samples_per_class = 10\n",
    "        samples = []\n",
    "        np.random.seed(0)\n",
    "        random.seed(0)\n",
    "        \n",
    "        for y in range(10):\n",
    "            idxs = np.nonzero(y_train == y)[0]\n",
    "            for i in range(samples_per_class):\n",
    "                idx = idxs[random.randrange(idxs.shape[0])]\n",
    "                samples.append(x_train[idx])\n",
    "        img = make_grid(np.expand_dims(np.array(samples), -1), ncols=samples_per_class)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    if fully_connected:\n",
    "        # data preprocessing for neural network with fully-connected layers\n",
    "        data = {\n",
    "            'X_train': np.array(x_train[:55000], np.float32).reshape((55000, -1)),  # training data\n",
    "            'y_train': np.array(y_train[:55000], np.int32),  # training labels\n",
    "            'X_val': np.array(x_train[55000:], np.float32).reshape((5000, -1)),  # validation data\n",
    "            'y_val': np.array(y_train[55000:], np.int32),  # validation labels\n",
    "            'X_test': x_test.reshape((num_test, -1)),  # test data\n",
    "            'y_test': y_test,  # test labels\n",
    "        }\n",
    "    else:\n",
    "        # data preprocessing for neural network with convolutional layers\n",
    "        data = {\n",
    "           'X_train': np.array(x_train[:55000], np.float32).reshape((55000, 1, 28, 28)),  # training data\n",
    "           'y_train': np.array(y_train[:55000], np.int32),  # training labels\n",
    "           'X_val': np.array(x_train[55000:], np.float32).reshape((5000, 1, 28, 28)),  # validation data\n",
    "           'y_val': np.array(y_train[55000:], np.int32),  # validation labels\n",
    "           'X_test': x_test,  # test data\n",
    "           'y_test': y_test,  # test labels\n",
    "        }\n",
    "    return data\n",
    "\n",
    "q5_data = process_mnist('data/two_layer_net/mnist.npz', visualize=True, fully_connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjAUalCBAOVd"
   },
   "source": [
    "### Train the network\n",
    "We first want to train our two layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "Wgw06cLXAOVd",
    "outputId": "be163c99-6590-4354-eafa-93d623bed3a8"
   },
   "outputs": [],
   "source": [
    "import solver\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "D = 28*28\n",
    "C = 10\n",
    "H = 100\n",
    "std = 1e-3\n",
    "\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "# the update rule of 'adam' can be used to replace 'sgd' if it is helpful.\n",
    "s = solver.Solver(model, q5_data,\n",
    "                       update_rule='sgd',\n",
    "                       optim_config={'learning_rate': 1e-3,},\n",
    "                       lr_decay=0.95,\n",
    "                       num_epochs=5, batch_size=100,\n",
    "                       verbose=False, print_every=10000)\n",
    "s.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss function and the accuracies on the training and validation sets during optimization. <span class=\"instruction\">Please report 'two_layer_net.png' file in your **writeup**.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(s.loss_history, '.')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(s.train_acc_history, '-o', label='train')\n",
    "plt.plot(s.val_acc_history, '-x', label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(14, 6)\n",
    "\n",
    "plt.savefig('two_layer_net.png', dpi=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to check the performance over the testset. (Our best model gets around 96% test-set accuracy -- did you beat us?) <span class=\"instruction\">Please report your test accuracy in your **writeup**</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = s.check_accuracy(X=np.array(q5_data['X_test'], np.float32), y=q5_data['y_test'])\n",
    "print('Test accuracy: {:.2f}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eYE9thuXn4zP",
    "CdowvtJen-IP",
    "KtMy3qeipNK3",
    "Hbe3wUpVAjma",
    "lJqim3P1qZgv",
    "ZLdCF3B-AOVT",
    "7XNJ3ydEAOVW",
    "vExP-7n3AOVa",
    "LjAUalCBAOVd",
    "8cPIajWNAOVg",
    "_CsYAv3uAOVi",
    "ixxgq5RKAOVl",
    "OlVbXxmPNzPY",
    "rDNZ8ZAnN7hj",
    "QpSrK3olUfOZ",
    "3zFWkxebWXtu",
    "mVCEro4FAOVq",
    "UG56gKWsAOVv",
    "37R_J2uMP3d-"
   ],
   "name": "two_layer_net.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
